{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dokumentation for model to estimate SWE from snow depth (SD) and meteorologcal data \n",
    "\n",
    "## Steps \n",
    "## 0. Create conda enviroment to install used packages\n",
    "## 1. Preparation of data\n",
    "## 2. Training of single MLP moder\n",
    "### 2a. Function for training of an ensemble of MLPs\n",
    "### 2b. Train MLP ensemble \n",
    "## 3 Evaluation of training\n",
    "### 3a. Function for ensemble evaluation\n",
    "### 3b. Evaluate on vaildation data set\n",
    "## 4. Multiple MLP model \n",
    "## 5. Simulation and evaluation on unseen data set\n",
    "### 5a. Simulate SWE \n",
    "### 5b. Evaluate simulation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----------------------------------------------------------------------------------------------------------------------------------\n",
    "## Required data: \n",
    "1. Snow depth (in cm) and SWE (in mm) + longitude, latitude, elevation of site locations \n",
    "2. Timeline of daily meteorological data associated with the snow depth records (timeline needs to cover a date range from the previous 1. of September till the date of the snow depth records)\n",
    "    - daily total precipitation (in mm)\n",
    "    - daily maximal temperature (in °C)\n",
    "    - daily minimal temperature (in °C)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -----------------------------------------------------------------------------------------------------------------------------\n",
    "## Required data shape\n",
    "1. Snow depth and SWE\n",
    "\t- 2 dim DataFrame\n",
    "\t- index: station ID\n",
    "\t- columns: date in datetime format \n",
    "2. Information of stations\n",
    "    - 2 dim DataFrame\n",
    "\t- index: station ID\n",
    "\t- columns: ['Lat', 'Long', 'Elev']\n",
    "3. Meteorological data \n",
    "\t- 2 dim DataFrame \n",
    "\t- index: station ID \n",
    "\t- columns: date in datetime format  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## -----------------------------------------------------------------------------------------------------------------------------\n",
    "## 0. Create conda enviroment to install used packages\n",
    "\n",
    "Builing a Conda enviroment to ensure all dependencies. The following needs to be typed in Anaconda Prompt.  \n",
    "\n",
    "#### Create Conda enviroment\n",
    "conda create -n SD2SWE python=3.6.10 <br>\n",
    "conda activate SD2SWE <br>\n",
    "\n",
    "#### Add packages \n",
    "conda install numpy=1.16.4 pandas geopandas matplotlib tensorflow scikit-learn ipykernel<br>\n",
    "\n",
    "#### Install spyder within the enviroment \n",
    "conda install spyder\n",
    "\n",
    "#### Add enviroment to jupyter notebook\n",
    "python -m ipykernel install --user --name=SD2SWE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----------------------------------------------------------------------------------------------------------------------------------\n",
    "## 1. Preparation of data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Snow survey data and meteorological data loaded\n"
     ]
    }
   ],
   "source": [
    "import pickle \n",
    "\n",
    "# load Canadian historical sonw survey (CHSS): dictionary with Station ID, longitude, latitude, elevation,\n",
    "#                                              2 dim matrix (Station ID x dates) for SWE, SD, density \n",
    "CHSS = pickle.load(open(\"00_saved_variables/ECCC_Nivo\", \"rb\"))\n",
    "\n",
    "# Load associated meteorological data \n",
    "total_precip = pickle.load(open(\"00_saved_variables/Data_Meteo_tp\", \"rb\"))\n",
    "tmin = pickle.load(open(\"00_saved_variables/Data_Meteo_tmin\", \"rb\"))\n",
    "tmax = pickle.load(open(\"00_saved_variables/Data_Meteo_tmax\", \"rb\"))\n",
    "\n",
    "print('Snow survey data and meteorological data loaded')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# correction of temperature because of different elevation of station and grid\n",
    "# load elevation of grid points \n",
    "elevation = pickle.load(open(\"00_saved_variables/elevation_ERA5\", \"rb\"))\n",
    "# round Longitude and Latitude to the first decimal to fit them together (ERA5 lowest resolution is 0.1x0.1)\n",
    "elevation.columns = np.round(elevation.columns, decimals=1)\n",
    "elevation.index = np.round(elevation.index, decimals=1)\n",
    "Lat_station = CHSS['Info_station']['Lat'].round(decimals=1)\n",
    "Long_station = CHSS['Info_station']['Long'].round(decimals=1)\n",
    "# apply temperature correction\n",
    "for i in Lat_station.index: \n",
    "    elev_grid = elevation.loc[Lat_station[i], Long_station[i]]\n",
    "    elev_st = CHSS['Info_station']['Elev(m)'].loc[i]\n",
    "    tmin.loc[i, :] = tmin.loc[i, :] + ((elev_st - elev_grid)/1000 * (-6))\n",
    "    tmax.loc[i, :] = tmax.loc[i, :] + ((elev_st - elev_grid)/1000 * (-6))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-26 13:55:18.508170\n",
      "2800 left till loop over stations done\n",
      "2020-03-26 14:18:02.340266\n",
      "2700 left till loop over stations done\n",
      "2020-03-26 14:40:33.789434\n",
      "2600 left till loop over stations done\n",
      "2020-03-26 15:04:27.427066\n",
      "2500 left till loop over stations done\n",
      "2020-03-26 15:35:25.319486\n",
      "2400 left till loop over stations done\n",
      "2020-03-26 15:53:27.168628\n",
      "2300 left till loop over stations done\n",
      "2020-03-26 16:10:11.505588\n",
      "2200 left till loop over stations done\n",
      "2020-03-26 16:21:04.919400\n",
      "2100 left till loop over stations done\n",
      "2020-03-26 16:44:35.187838\n",
      "2000 left till loop over stations done\n",
      "2020-03-26 17:13:35.775319\n",
      "1900 left till loop over stations done\n",
      "2020-03-26 17:38:02.272024\n",
      "1800 left till loop over stations done\n",
      "2020-03-26 18:04:39.538401\n",
      "1700 left till loop over stations done\n",
      "2020-03-26 18:40:23.080082\n",
      "1600 left till loop over stations done\n",
      "2020-03-26 19:16:47.213548\n",
      "1500 left till loop over stations done\n",
      "2020-03-26 19:54:14.341491\n",
      "1400 left till loop over stations done\n",
      "2020-03-26 20:31:02.907117\n",
      "1300 left till loop over stations done\n",
      "2020-03-26 21:09:49.244927\n",
      "1200 left till loop over stations done\n",
      "2020-03-26 21:48:31.333687\n",
      "1100 left till loop over stations done\n",
      "2020-03-26 22:25:02.277998\n",
      "1000 left till loop over stations done\n",
      "2020-03-26 23:21:06.614157\n",
      "900 left till loop over stations done\n",
      "2020-03-26 23:34:26.079027\n",
      "800 left till loop over stations done\n",
      "2020-03-26 23:49:12.113943\n",
      "700 left till loop over stations done\n",
      "2020-03-27 00:03:14.623464\n",
      "600 left till loop over stations done\n",
      "2020-03-27 00:17:22.140971\n",
      "500 left till loop over stations done\n",
      "2020-03-27 00:33:55.686212\n",
      "400 left till loop over stations done\n",
      "2020-03-27 00:49:36.354698\n",
      "300 left till loop over stations done\n",
      "2020-03-27 01:09:11.013282\n",
      "200 left till loop over stations done\n",
      "2020-03-27 01:43:50.157926\n",
      "100 left till loop over stations done\n",
      "2020-03-27 02:05:06.787352\n",
      "0 left till loop over stations done\n"
     ]
    }
   ],
   "source": [
    "# load function to calculate input varibles from meteorological data \n",
    "import input_calc\n",
    "import pandas as pd\n",
    "from datetime import datetime, timedelta\n",
    "\n",
    "# create dataset for input; \n",
    "# one line represents one records of SD and the associated input varibles, which will be calculated in the following \n",
    "MLP_data = pd.DataFrame(columns=['StationID', 'Date', 'Den', 'SD', 'SWE', 'Elev', 'Lat', 'Long',\n",
    "                                 'Day of year', 'days without snow', 'number frost-defrost',\n",
    "                                 'accum pos degrees', 'average age SC', 'number layer',\n",
    "                                 'accum solid precip', 'accum solid precip in last 10 days',\n",
    "                                 'total precip last 10 days', 'average temp last 6 days'])\n",
    "\n",
    "start_year = tmin.columns[0].year\n",
    "end_year = tmin.columns[-1].year\n",
    "range_year = np.arange(start_year + 1, end_year + 1)\n",
    "dates_Meteo = tmin.columns\n",
    "nb_dates = len(dates_Meteo)\n",
    "\n",
    "# count to print how may stations are left \n",
    "count = len(CHSS['Info_station'].index)\n",
    "line = 0\n",
    "for St in CHSS['Info_station'].index:\n",
    "    count = count - 1\n",
    "    if count % 100 == 0: \n",
    "        print(datetime.now())\n",
    "        print('{} left till loop over stations done'.format(count))\n",
    "    # calculate average temeperature\n",
    "    tmid = (tmin.loc[St,:] + tmax.loc[St,:])/2\n",
    "    \n",
    "    # logistic regression to separate precipitation into silod and liquid parts \n",
    "    # tested over the northern hemnisphere by\n",
    "    # ('Spatial variation of the rain–snow temperature threshold across the Northern Hemisphere' Jennings et al. 2018)\n",
    "    probab_snow = 1 / (1 + np.exp(-1.54 + 1.24 * tmid))\n",
    "    total_precip_solid = total_precip.loc[St,:] * probab_snow\n",
    "    \n",
    "    # generate frost-defrost timeline for concerning station; \n",
    "    # -1°C for the maximal temp and 1°C for the minimal temperature are the threshold for freezing and thawing, respectively\n",
    "    frost_defrost_vect = input_calc.frost_defrost(tmin.loc[St,:], tmax.loc[St,:], dates_Meteo, nb_dates)\n",
    "    # generate timeline of number of days without snow for concerning station\n",
    "    nb_days_without_snow_vect = input_calc.num_without_snow(tmax.loc[St,:], total_precip.loc[St,:], dates_Meteo, nb_dates)\n",
    "    # generate timeline of accumulated posiitve degrees since beginning of winter (1. of September)\n",
    "    pos_degree_vect = input_calc.pos_degrees(tmid, dates_Meteo, nb_dates)\n",
    "    # generate timeline of number of layers;\n",
    "    # a new layer is considered to be created if there is a 3-days gap\n",
    "    # with less than 10mm of (accumulated over these 3 days) solid precipitation\n",
    "    num_layer_vec = input_calc.num_layer(3, 10, total_precip_solid, dates_Meteo, nb_dates)\n",
    "    \n",
    "    # loop over the dates for the concerning station\n",
    "    for dat in CHSS['SWE(mm)'].columns:\n",
    "        if ~np.isnan(CHSS['SWE(mm)'].loc[St, dat]):\n",
    "            # preparation\n",
    "            ndRow = np.empty((1,len(MLP_data.columns)))\n",
    "            ndRow[:] = np.nan \n",
    "            row = pd.DataFrame(ndRow, columns=MLP_data.columns)\n",
    "            MLP_data = MLP_data.append(row, ignore_index=True)\n",
    "\n",
    "            # add Station ID\n",
    "            MLP_data.iloc[line]['StationID'] = St\n",
    "            # add Lat and Long \n",
    "            MLP_data.iloc[line]['Lat'] = CHSS['Info_station'].loc[St, 'Lat']\n",
    "            MLP_data.iloc[line]['Long'] = CHSS['Info_station'].loc[St, 'Long']\n",
    "            # add date of measurement\n",
    "            MLP_data.iloc[line]['Date'] = dat\n",
    "            # add station elevation\n",
    "            MLP_data.iloc[line]['Elev'] =  CHSS['Info_station'].loc[St, 'Elev(m)']\n",
    "            # add snow bulk denisty \n",
    "            MLP_data.iloc[line]['Den'] = CHSS['Den(kg/m3)'].loc[St, dat]\n",
    "            # add SWE\n",
    "            MLP_data.iloc[line]['SWE'] = CHSS['SWE(mm)'].loc[St, dat]\n",
    "            # add snow depth\n",
    "            MLP_data.iloc[line]['SD'] = CHSS['SD(cm)'].loc[St, dat]\n",
    "            # days without snow since 1st of august \n",
    "            MLP_data.iloc[line]['days without snow'] = nb_days_without_snow_vect.loc[dat][0]\n",
    "            # number of frost-defrost events since 1st of September \n",
    "            MLP_data.iloc[line]['number frost-defrost'] = frost_defrost_vect.loc[dat][0]\n",
    "            # calculate the accumulated temperature from 1st of September till record\n",
    "            MLP_data.iloc[line]['accum pos degrees'] = pos_degree_vect.loc[dat][0]\n",
    "            # calculate the average age of the snow cover\n",
    "            MLP_data.iloc[line]['average age SC'], total_precip_mod_solid, cumul, nb_days = input_calc.age_snow_cover(dat, tmid, total_precip_solid)\n",
    "            # add number of days since 1st of September\n",
    "            MLP_data.iloc[line]['Day of year'] = nb_days\n",
    "            # estimate the number of layers in the snow cover\n",
    "            MLP_data.iloc[line]['number layer'] = num_layer_vec.loc[dat][0]\n",
    "            # calculate accumlated solid precipitation from 1st September till record\n",
    "            MLP_data.iloc[line]['accum solid precip'] = cumul\n",
    "            # calculate accumlated solid precipitation in the last 10 days before the record\n",
    "            MLP_data.iloc[line]['accum solid precip in last 10 days'] = np.sum(total_precip_mod_solid[-10:])\n",
    "            # calculate accumlated total precipitation in the last 10 days before the record\n",
    "            dat_Ndays = dat - timedelta(days = 10)\n",
    "            MLP_data.iloc[line]['total precip last 10 days'] = np.sum(total_precip.loc[St, dat:dat_Ndays:-1])\n",
    "            # calculate average temperature in the last 6 days before the record\n",
    "            dat_Ndays = dat - timedelta(days = 6)\n",
    "            MLP_data.iloc[line][ 'average temp last 6 days'] = np.mean(tmid.loc[dat:dat_Ndays:-1])           \n",
    "            # increase line count\n",
    "            line += 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add snow class (snow classification definde by Sturm et al. 2009)\n",
    "The map of the snow classes is defined on a 0.5x0.5 grid. Stations close to the shore are defined as 'water'. 'Ice' snow class contains only 124 records. If records is defined as 'water' or 'ice', the closest nearby snow class of the remaining is defined. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-27 12:35:06.283849\n",
      "2800 left till loop over stations done\n",
      "2020-03-27 12:35:12.733697\n",
      "2700 left till loop over stations done\n",
      "2020-03-27 12:35:18.102399\n",
      "2600 left till loop over stations done\n",
      "2020-03-27 12:35:23.132935\n",
      "2500 left till loop over stations done\n",
      "2020-03-27 12:35:27.980065\n",
      "2400 left till loop over stations done\n",
      "2020-03-27 12:35:32.280061\n",
      "2300 left till loop over stations done\n",
      "2020-03-27 12:35:36.190372\n",
      "2200 left till loop over stations done\n",
      "2020-03-27 12:35:40.354570\n",
      "2100 left till loop over stations done\n",
      "2020-03-27 12:35:44.592284\n",
      "2000 left till loop over stations done\n",
      "2020-03-27 12:35:48.557781\n",
      "1900 left till loop over stations done\n",
      "2020-03-27 12:35:52.630005\n",
      "1800 left till loop over stations done\n",
      "2020-03-27 12:35:56.644075\n",
      "1700 left till loop over stations done\n",
      "2020-03-27 12:36:01.585871\n",
      "1600 left till loop over stations done\n",
      "2020-03-27 12:36:06.928649\n",
      "1500 left till loop over stations done\n",
      "2020-03-27 12:36:11.989286\n",
      "1400 left till loop over stations done\n",
      "2020-03-27 12:36:16.852620\n",
      "1300 left till loop over stations done\n",
      "2020-03-27 12:36:21.739915\n",
      "1200 left till loop over stations done\n",
      "2020-03-27 12:36:25.749381\n",
      "1100 left till loop over stations done\n",
      "2020-03-27 12:36:29.933666\n",
      "1000 left till loop over stations done\n",
      "2020-03-27 12:36:34.059262\n",
      "900 left till loop over stations done\n",
      "2020-03-27 12:36:38.004661\n",
      "800 left till loop over stations done\n",
      "2020-03-27 12:36:41.959768\n",
      "700 left till loop over stations done\n",
      "2020-03-27 12:36:45.841527\n",
      "600 left till loop over stations done\n",
      "2020-03-27 12:36:49.903968\n",
      "500 left till loop over stations done\n",
      "2020-03-27 12:36:53.904760\n",
      "400 left till loop over stations done\n",
      "2020-03-27 12:36:58.411652\n",
      "300 left till loop over stations done\n",
      "2020-03-27 12:37:02.586924\n",
      "200 left till loop over stations done\n",
      "2020-03-27 12:37:06.309790\n",
      "100 left till loop over stations done\n",
      "2020-03-27 12:37:10.367233\n",
      "0 left till loop over stations done\n"
     ]
    }
   ],
   "source": [
    "from shapely.geometry import Point\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "CHSS = pickle.load(open(\"00_saved_variables/ECCC_Nivo\", \"rb\"))\n",
    "MLP_data = pickle.load(open(\"00_saved_variables/MLP_input\", \"rb\"))\n",
    "\n",
    "# load snowclasses and change projection to WGS84 (unit in lat/long)\n",
    "SC_Canada = pickle.load(open(\"00_saved_variables/SC_Canada\", \"rb\"))\n",
    "SC_Canada.crs = '+proj=robin +lon_0=0 +x_0=0 +y_0=0 +datum=WGS84 +units=m +no_defs'\n",
    "SC_Canada = SC_Canada.to_crs({'init' :'epsg:4326'})\n",
    "\n",
    "# initialise count to print how may records are left \n",
    "count = CHSS['Info_station'].shape[0]\n",
    "# add snow class to each record\n",
    "MLP_data['snowclass'] = np.nan\n",
    "for st in CHSS['Info_station'].index:\n",
    "    count = count - 1\n",
    "    if count % 100 == 0:\n",
    "        print(datetime.now())\n",
    "        print('{} left till loop over stations done'.format(count))\n",
    "    # find all records associated with this station\n",
    "    idx_st = [MLP_data['StationID'] == st][0]\n",
    "    # create point and find out in which snow class in lies in\n",
    "    pt = Point(CHSS['Info_station']['Long'][st], CHSS['Info_station']['Lat'][st])\n",
    "    for j in range(len(SC_Canada)):\n",
    "        if pt.intersects(SC_Canada['geometry'][j]):\n",
    "            # case, if snow class is 'water', look for closest snow class nearby which is not 'ice'\n",
    "            if SC_Canada['snow type'][j] == 'Water':\n",
    "                dist = pd.Series(index=SC_Canada['snow type'], dtype=\"float64\")\n",
    "                for l in range(len(SC_Canada)):\n",
    "                    dist[SC_Canada['snow type'][l]] = pt.distance(SC_Canada['geometry'][l])\n",
    "                dist = dist.drop('Water', axis=0)\n",
    "                nearest = dist.loc[dist == dist.min()]\n",
    "                if nearest.index[0] == 'Ice': \n",
    "                    dist = dist.drop('Ice', axis=0)\n",
    "                    nearest = dist.loc[dist == dist.min()]\n",
    "                MLP_data.loc[idx_st, 'snowclass'] = nearest.index[0]\n",
    "                break\n",
    "            \n",
    "            # case, if snow class is 'ice', look for closest snow class nearby which is not 'water' \n",
    "            elif SC_Canada['snow type'][j] == 'Ice':\n",
    "                dist = pd.Series(index=SC_Canada['snow type'], dtype=\"float64\")\n",
    "                for l in range(len(SC_Canada)):\n",
    "                    dist[SC_Canada['snow type'][l]] = pt.distance(SC_Canada['geometry'][l])\n",
    "                dist = dist.drop('Ice', axis=0)\n",
    "                nearest = dist.loc[dist == dist.min()]\n",
    "                if nearest.index[0] == 'Water': \n",
    "                    dist = dist.drop('Water', axis=0)\n",
    "                    nearest = dist.loc[dist == dist.min()]\n",
    "                MLP_data.loc[idx_st, 'snowclass'] = nearest.index[0] \n",
    "                break\n",
    "                \n",
    "            # case, if snow class is neither 'water' nor 'ice'\n",
    "            else:\n",
    "                MLP_data.loc[idx_st, 'snowclass'] = SC_Canada['snow type'][j]\n",
    "                break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data split into three sub-data sets by random draw\n",
    "    - Training data set: used to train the MLPs in the ensemble \n",
    "    - Validation data set: used to validate the optimisation \n",
    "    - testing data set: used to test the final model on an unseen data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into 3 data sets \n",
    "data_train = MLP_data.sample(frac=0.334,random_state=0)\n",
    "data_remain = MLP_data.drop(data_train.index)\n",
    "data_test = data_remain.sample(frac=0.5,random_state=1)\n",
    "data_val = data_remain.drop(data_test.index)\n",
    "\n",
    "pickle.dump(data_train, open(\"00_saved_variables/data_train\", \"wb\"))   \n",
    "pickle.dump(data_test, open(\"00_saved_variables/data_test\", \"wb\"))   \n",
    "pickle.dump(data_val, open(\"00_saved_variables/data_val\", \"wb\"))   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perturb snow depth in the training data set according to WMO recommondations \n",
    "    - if SD smaller 20cm: highest error +/- 1 cm \n",
    "    - is SD larger 20cm: highest error +/- 20%\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020-03-27 12:38:32.280249\n",
      "70000 left till loop over records done\n",
      "2020-03-27 12:38:35.124739\n",
      "60000 left till loop over records done\n",
      "2020-03-27 12:38:37.920534\n",
      "50000 left till loop over records done\n",
      "2020-03-27 12:38:40.916562\n",
      "40000 left till loop over records done\n",
      "2020-03-27 12:38:43.715130\n",
      "30000 left till loop over records done\n",
      "2020-03-27 12:38:46.517065\n",
      "20000 left till loop over records done\n",
      "2020-03-27 12:38:49.411460\n",
      "10000 left till loop over records done\n",
      "2020-03-27 12:38:52.481890\n",
      "0 left till loop over records done\n"
     ]
    }
   ],
   "source": [
    "# copy records 20 times\n",
    "data_train_noise = data_train.iloc[np.repeat(np.arange(len(data_train)), 20)].copy()\n",
    "\n",
    "# initialise vector for perturbed SD \n",
    "SD_noise = np.empty(len(data_train.index)*20)\n",
    "\n",
    "# initialise count to print how may records are left \n",
    "count = data_train.shape[0]\n",
    "for j in range(data_train.shape[0]):\n",
    "    count = count - 1\n",
    "    if count % 10000 == 0:\n",
    "        print(datetime.now())\n",
    "        print('{} left till loop over records done'.format(count))\n",
    "    SD = data_train.iloc[j]['SD']\n",
    "    if SD < 20: \n",
    "        SD_low = SD - 1\n",
    "        SD_high = SD + 1\n",
    "    else: \n",
    "        SD_low = SD * 0.95\n",
    "        SD_high = SD * 1.05\n",
    "    SD_noise_1rec = np.random.uniform(low=SD_low, high=SD_high, size=20)\n",
    "    SD_noise[j*20:(j+1)*20] = SD_noise_1rec\n",
    "    \n",
    "data_train_noise['SD'] = SD_noise\n",
    "pickle.dump(data_train_noise, open(\"00_saved_variables/data_train_perturbed\", \"wb\"))  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----------------------------------------------------------------------------------------------------------------------------------\n",
    "## 2. Training of single MLP model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2a. Function for training of an ensemble \n",
    "The following function trains an ensmeble of MLPs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#inputs: x_train: input for training \n",
    "#        y_train: target for training \n",
    "#        x_val: input for MLP validation\n",
    "#        y_val: target for MLP validation\n",
    "#        nb_epochs: number of epochs in training \n",
    "#        nb_member: number of members in ensemble \n",
    "#        nb_hid: number of neurons in hidden layer \n",
    "#        save_path: path to save the neural networks for later use \n",
    "#        activ_fc: activation function in hidden layer\n",
    "#                  'tanh', 'ReLU', 'LeakyReLU' are possible \n",
    "#        init_w: float value for weights initialisation with U(-init_w, init_w)\n",
    "#        init_b: float value for weights initialisation with U(-init_b, init_b)\n",
    "#        potAlg: determine optimisation Algorithm: 'Adadelta' and 'RMSProp' are possible\n",
    "#        batch_size: number of records in one minibatch \n",
    "#        shuf_data: 0 = no shuffling of the data between the epochs\n",
    "#                   1 = shuffling of the data between the epochs\n",
    "#    \n",
    "#outputs: final: results of network in ensemble by using the x_test data as input\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "# -------------------------------------------------------------------------------------------------\n",
    "\n",
    "from datetime import datetime \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle \n",
    "\n",
    "\n",
    "def MLP(x_train, x_val, y_train, y_val, nb_epochs, nb_members, nb_hid, save_path, \n",
    "        activ_fc, init_w, init_b, optAlg, batch_size, shuf_data):\n",
    "    \n",
    "    # standardize data \n",
    "    # create scaler\n",
    "    scalerIn = StandardScaler()\n",
    "    scalerOut = StandardScaler()\n",
    "    # fit scaler on dataIn and target and transform it\n",
    "    scalerIn.fit(x_train)\n",
    "    x_train_std = scalerIn.transform(x_train)\n",
    "    x_val_std = scalerIn.transform(x_val)\n",
    "    scalerOut.fit(y_train.reshape(-1, 1))\n",
    "    y_train_std = scalerOut.transform(y_train.reshape(-1, 1))\n",
    "    y_val_std = scalerOut.transform(y_val.reshape(-1, 1))        \n",
    "  \n",
    "    # train an ensemble of nb_members members and save simulation in matrix \"final\"\n",
    "    final = np.empty((len(y_val_std), nb_members))\n",
    "    \n",
    "    # Optimisation Parameters\n",
    "    if optAlg == 'Adadelta':\n",
    "        learning_rate = 1\n",
    "    elif optAlg == 'RMSProp':\n",
    "        learning_rate = 0.00001\n",
    "    decay=0.9\n",
    "    momentum=0.0\n",
    "    epsilon=1e-8\n",
    "    display_step = 1\n",
    "    # if batch size None, then use entrie batch \n",
    "    if batch_size == None: \n",
    "        batch_size = len(y_train_std)\n",
    "    \n",
    "    # Network Parameters\n",
    "    nb_input = x_train.shape[1]\n",
    "    nb_output = 1 \n",
    "    \n",
    "    # tf Graph input\n",
    "    X = tf.placeholder(\"float\", [None, nb_input],name=\"input\")\n",
    "    Y = tf.placeholder(\"float\", [None, nb_output])\n",
    "    \n",
    "    # Store layers weight & bias\n",
    "    weights = {\n",
    "        'h1': tf.Variable(tf.random_uniform([nb_input, nb_hid], minval=-init_w, maxval=init_w), name='Whid'),\n",
    "        'out': tf.Variable(tf.random_uniform([nb_hid, nb_output], minval=-init_w, maxval=init_w), name='Wout')\n",
    "    }\n",
    "    biases = {\n",
    "        'b1': tf.Variable(tf.random_uniform([nb_hid], minval=-init_b, maxval=init_b), name='Bhid'),\n",
    "        'out': tf.Variable(tf.random_uniform([nb_output], minval=-init_b, maxval=init_b), name='Bout')\n",
    "    }\n",
    "    \n",
    "    \n",
    "    # Create model\n",
    "    def multilayer_perceptron(x):\n",
    "        # Hidden fully connected layer with 256 neurons\n",
    "        if activ_fc == 'tanh':\n",
    "            layer_1 = tf.tanh(tf.add(tf.matmul(x, weights['h1']), biases['b1']))\n",
    "        elif activ_fc == 'ReLU':\n",
    "            layer_1 = tf.nn.relu(tf.add(tf.matmul(x, weights['h1']), biases['b1']))\n",
    "        elif activ_fc == 'Leaky_ReLU':\n",
    "            layer_1 = tf.nn.leaky_relu(tf.add(tf.matmul(x, weights['h1']), biases['b1']), alpha=0.1)\n",
    "        # Output fully connected layer with a neuron for each class\n",
    "        out_layer = tf.add(tf.matmul(layer_1, weights['out']), biases['out'], name=\"op_to_restore\")\n",
    "        return out_layer\n",
    "    \n",
    "    # Construct model\n",
    "    output = multilayer_perceptron(X)\n",
    "    \n",
    "    # Original loss function\n",
    "    loss_op = tf.reduce_mean(tf.squared_difference(output, Y))\n",
    "    if optAlg == 'Adadelta':\n",
    "        optimizer = tf.train.AdadeltaOptimizer(learning_rate=learning_rate, rho=decay, epsilon=epsilon)\n",
    "    elif optAlg == 'RMSProp':\n",
    "        optimizer = tf.train.RMSPropOptimizer(learning_rate=learning_rate, decay=decay, momentum=momentum, epsilon=epsilon)\n",
    "    train_op = optimizer.minimize(loss_op)\n",
    "    \n",
    "    #Create a saver object which will save all the variables\n",
    "    saver = tf.train.Saver(max_to_keep=nb_members)\n",
    "    \n",
    "      \n",
    "    for mb in range(nb_members):\n",
    "        print('member {}'.format(mb))\n",
    "        print(datetime.now())\n",
    "        # Initializing the variables\n",
    "        init = tf.global_variables_initializer()\n",
    "        \n",
    "        with tf.Session() as sess:\n",
    "            sess.run(init)\n",
    "        \n",
    "            # Training cycle\n",
    "            for epoch in range(nb_epochs):\n",
    "                # shuffle data if required\n",
    "                if shuf_data == 1: \n",
    "                    p = np.random.permutation(len(x_train_std))\n",
    "                    x_train_pt = x_train_std[p]\n",
    "                    y_train_pt = y_train_std[p]\n",
    "                \n",
    "\n",
    "                total_batch = int(len(y_train_std)/batch_size)\n",
    "                # Loop over all minibatches\n",
    "                for i in range(total_batch):\n",
    "                    batch_x = x_train_pt[i*batch_size:(i+1)*batch_size, :]\n",
    "                    batch_y = y_train_pt[i*batch_size:(i+1)*batch_size]\n",
    "                    # Run optimization op (backprop) and cost op (to get loss value)\n",
    "                    _, c_train = sess.run([train_op, loss_op], feed_dict={X: batch_x, Y: batch_y})\n",
    "                    \n",
    "                # calculate the loss on test set \n",
    "                y_predict_std = sess.run(output, feed_dict={X: x_val_std}).flatten()\n",
    "                y_predict = scalerOut.inverse_transform(y_predict_std)   \n",
    "                c_val = np.mean((y_predict - y_val)**2)\n",
    "                # calculate the loss on train set \n",
    "                y_predict_std = sess.run(output, feed_dict={X: x_train_std}).flatten()\n",
    "                y_predict = scalerOut.inverse_transform(y_predict_std)   \n",
    "                c_train = np.mean((y_predict - y_train)**2)\n",
    "\n",
    "                # Display logs per epoch step\n",
    "                if epoch % display_step == 0:\n",
    "                    print(\"Epoch:\", '%04d' % (epoch+1), \"cost train={:.2f}\".format(c_train), \"cost test={:.2f}\".format(c_val))\n",
    "            print(\"Optimization Finished!\")\n",
    "        \n",
    "            # predict outcome \n",
    "            y_predict_std = sess.run(output, feed_dict={X: x_val_std}).flatten()\n",
    "            y_predict = scalerOut.inverse_transform(y_predict_std)    \n",
    "            final[:,mb] = y_predict.flatten()\n",
    "            \n",
    "            # save trained model\n",
    "            saver.save(sess, save_path + '/mb_{}.ckpt'.format(mb))\n",
    "             \n",
    "            sess.close()\n",
    "            \n",
    "    # save scaler         \n",
    "    pickle.dump([scalerIn, scalerOut], open(save_path + '/scaler', \"wb\"))\n",
    "    \n",
    "    print('training of ensemble finished')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2b. Train MLP ensemble "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\konst\\anaconda3\\envs\\test4\\lib\\site-packages\\tensorflow\\python\\framework\\op_def_library.py:263: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "member 0\n",
      "2020-03-27 12:41:43.179350\n",
      "Epoch: 0001 cost train=38277.71 cost test=39455.57\n",
      "Epoch: 0002 cost train=9777.57 cost test=10005.42\n",
      "Epoch: 0003 cost train=5823.44 cost test=5919.92\n",
      "Epoch: 0004 cost train=4878.50 cost test=4941.02\n",
      "Epoch: 0005 cost train=4344.25 cost test=4384.37\n",
      "Optimization Finished!\n",
      "member 1\n",
      "2020-03-27 12:43:07.111901\n",
      "Epoch: 0001 cost train=47421.86 cost test=50470.04\n",
      "Epoch: 0002 cost train=10847.99 cost test=11327.60\n",
      "Epoch: 0003 cost train=6175.25 cost test=6331.25\n",
      "Epoch: 0004 cost train=5031.14 cost test=5089.92\n",
      "Epoch: 0005 cost train=4586.65 cost test=4635.65\n",
      "Optimization Finished!\n",
      "member 2\n",
      "2020-03-27 12:45:06.324727\n",
      "Epoch: 0001 cost train=40233.61 cost test=42018.58\n",
      "Epoch: 0002 cost train=9968.61 cost test=10377.29\n",
      "Epoch: 0003 cost train=6214.31 cost test=6328.14\n",
      "Epoch: 0004 cost train=5186.36 cost test=5266.73\n",
      "Epoch: 0005 cost train=4684.06 cost test=4739.27\n",
      "Optimization Finished!\n",
      "member 3\n",
      "2020-03-27 12:47:36.587915\n",
      "Epoch: 0001 cost train=38131.15 cost test=39407.52\n",
      "Epoch: 0002 cost train=9348.26 cost test=9635.84\n",
      "Epoch: 0003 cost train=5816.23 cost test=5982.07\n",
      "Epoch: 0004 cost train=4914.75 cost test=5053.26\n",
      "Epoch: 0005 cost train=4526.91 cost test=4641.28\n",
      "Optimization Finished!\n",
      "member 4\n",
      "2020-03-27 12:50:01.597653\n",
      "Epoch: 0001 cost train=38739.29 cost test=40305.67\n",
      "Epoch: 0002 cost train=9134.27 cost test=9384.74\n",
      "Epoch: 0003 cost train=5509.09 cost test=5660.41\n",
      "Epoch: 0004 cost train=4593.31 cost test=4720.89\n",
      "Epoch: 0005 cost train=4243.17 cost test=4348.48\n",
      "Optimization Finished!\n",
      "member 5\n",
      "2020-03-27 12:52:29.015263\n",
      "Epoch: 0001 cost train=39583.51 cost test=41660.89\n",
      "Epoch: 0002 cost train=10785.46 cost test=11355.41\n",
      "Epoch: 0003 cost train=6164.27 cost test=6483.48\n",
      "Epoch: 0004 cost train=4879.02 cost test=5103.68\n",
      "Epoch: 0005 cost train=4395.24 cost test=4538.10\n",
      "Optimization Finished!\n",
      "member 6\n",
      "2020-03-27 12:54:53.432770\n",
      "Epoch: 0001 cost train=39149.76 cost test=40493.39\n",
      "Epoch: 0002 cost train=9732.38 cost test=10134.30\n",
      "Epoch: 0003 cost train=5894.99 cost test=6085.10\n",
      "Epoch: 0004 cost train=4893.35 cost test=5021.61\n",
      "Epoch: 0005 cost train=4325.44 cost test=4464.30\n",
      "Optimization Finished!\n",
      "member 7\n",
      "2020-03-27 12:57:18.679283\n",
      "Epoch: 0001 cost train=39244.82 cost test=40656.44\n",
      "Epoch: 0002 cost train=9153.53 cost test=9461.55\n",
      "Epoch: 0003 cost train=5593.41 cost test=5732.46\n",
      "Epoch: 0004 cost train=4708.99 cost test=4808.42\n",
      "Epoch: 0005 cost train=4346.99 cost test=4429.08\n",
      "Optimization Finished!\n",
      "member 8\n",
      "2020-03-27 12:59:42.051469\n",
      "Epoch: 0001 cost train=54693.65 cost test=56488.39\n",
      "Epoch: 0002 cost train=12618.02 cost test=12981.92\n",
      "Epoch: 0003 cost train=6465.49 cost test=6638.02\n",
      "Epoch: 0004 cost train=5060.89 cost test=5159.42\n",
      "Epoch: 0005 cost train=4463.88 cost test=4518.34\n",
      "Optimization Finished!\n",
      "member 9\n",
      "2020-03-27 13:02:08.355763\n",
      "Epoch: 0001 cost train=32335.76 cost test=33179.05\n",
      "Epoch: 0002 cost train=8470.04 cost test=8740.57\n",
      "Epoch: 0003 cost train=5530.02 cost test=5701.18\n",
      "Epoch: 0004 cost train=4733.67 cost test=4853.20\n",
      "Epoch: 0005 cost train=4360.50 cost test=4473.27\n",
      "Optimization Finished!\n",
      "member 10\n",
      "2020-03-27 13:04:31.256945\n",
      "Epoch: 0001 cost train=45139.48 cost test=46623.02\n",
      "Epoch: 0002 cost train=10738.57 cost test=11179.07\n",
      "Epoch: 0003 cost train=6112.91 cost test=6301.88\n",
      "Epoch: 0004 cost train=4932.18 cost test=5025.41\n",
      "Epoch: 0005 cost train=4481.41 cost test=4539.36\n",
      "Optimization Finished!\n",
      "member 11\n",
      "2020-03-27 13:06:57.968213\n",
      "Epoch: 0001 cost train=50235.75 cost test=52380.48\n",
      "Epoch: 0002 cost train=11405.83 cost test=11981.64\n",
      "Epoch: 0003 cost train=6390.11 cost test=6580.43\n",
      "Epoch: 0004 cost train=5042.24 cost test=5150.43\n",
      "Epoch: 0005 cost train=4490.14 cost test=4567.50\n",
      "Optimization Finished!\n",
      "member 12\n",
      "2020-03-27 13:09:26.424220\n",
      "Epoch: 0001 cost train=51123.85 cost test=52445.76\n",
      "Epoch: 0002 cost train=10898.32 cost test=11220.32\n",
      "Epoch: 0003 cost train=6027.57 cost test=6181.98\n",
      "Epoch: 0004 cost train=4850.86 cost test=5000.23\n",
      "Epoch: 0005 cost train=4414.52 cost test=4581.67\n",
      "Optimization Finished!\n",
      "member 13\n",
      "2020-03-27 13:10:18.352758\n",
      "Epoch: 0001 cost train=57770.43 cost test=59719.85\n",
      "Epoch: 0002 cost train=13003.88 cost test=13634.03\n",
      "Epoch: 0003 cost train=6839.73 cost test=7059.92\n",
      "Epoch: 0004 cost train=5243.02 cost test=5398.18\n",
      "Epoch: 0005 cost train=4665.04 cost test=4828.06\n",
      "Optimization Finished!\n",
      "member 14\n",
      "2020-03-27 13:10:57.976123\n",
      "Epoch: 0001 cost train=31538.13 cost test=32946.39\n",
      "Epoch: 0002 cost train=8324.00 cost test=8605.74\n",
      "Epoch: 0003 cost train=5587.67 cost test=5697.82\n",
      "Epoch: 0004 cost train=4746.88 cost test=4786.02\n",
      "Epoch: 0005 cost train=4412.23 cost test=4460.36\n",
      "Optimization Finished!\n",
      "member 15\n",
      "2020-03-27 13:11:40.584989\n",
      "Epoch: 0001 cost train=49027.24 cost test=50755.46\n",
      "Epoch: 0002 cost train=10906.90 cost test=11259.87\n",
      "Epoch: 0003 cost train=6259.10 cost test=6409.04\n",
      "Epoch: 0004 cost train=5110.11 cost test=5220.13\n",
      "Epoch: 0005 cost train=4484.12 cost test=4583.87\n",
      "Optimization Finished!\n",
      "member 16\n",
      "2020-03-27 13:12:29.542907\n",
      "Epoch: 0001 cost train=45925.23 cost test=48027.52\n",
      "Epoch: 0002 cost train=10753.17 cost test=10992.65\n",
      "Epoch: 0003 cost train=6055.78 cost test=6094.83\n",
      "Epoch: 0004 cost train=4905.68 cost test=4953.26\n",
      "Epoch: 0005 cost train=4399.82 cost test=4485.74\n",
      "Optimization Finished!\n",
      "member 17\n",
      "2020-03-27 13:13:27.483096\n",
      "Epoch: 0001 cost train=37933.91 cost test=39196.18\n",
      "Epoch: 0002 cost train=9393.27 cost test=9868.67\n",
      "Epoch: 0003 cost train=5787.21 cost test=6036.66\n",
      "Epoch: 0004 cost train=4793.65 cost test=4962.39\n",
      "Epoch: 0005 cost train=4343.29 cost test=4478.58\n",
      "Optimization Finished!\n",
      "member 18\n",
      "2020-03-27 13:14:15.007370\n",
      "Epoch: 0001 cost train=43884.66 cost test=44620.50\n",
      "Epoch: 0002 cost train=11126.52 cost test=11509.14\n",
      "Epoch: 0003 cost train=6254.55 cost test=6430.72\n",
      "Epoch: 0004 cost train=4841.71 cost test=4932.51\n",
      "Epoch: 0005 cost train=4224.02 cost test=4346.44\n",
      "Optimization Finished!\n",
      "member 19\n",
      "2020-03-27 13:15:00.176538\n",
      "Epoch: 0001 cost train=36208.44 cost test=37261.89\n",
      "Epoch: 0002 cost train=9074.16 cost test=9378.02\n",
      "Epoch: 0003 cost train=5638.61 cost test=5764.37\n",
      "Epoch: 0004 cost train=4664.67 cost test=4763.62\n",
      "Epoch: 0005 cost train=4228.71 cost test=4346.07\n",
      "Optimization Finished!\n",
      "training of ensemble finished\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "import os\n",
    "import shutil\n",
    "\n",
    "# load train and validation data \n",
    "train = pickle.load(open(\"00_saved_variables/data_train_perturbed\", \"rb\"))\n",
    "val = pickle.load(open(\"00_saved_variables/data_val\", \"rb\"))\n",
    "\n",
    "# determine variables used for MLP optimisation \n",
    "variables = ['SD', 'SWE', 'Day of year', 'days without snow', 'number frost-defrost',\n",
    "             'accum pos degrees', 'average age SC', 'number layer', 'accum solid precip',\n",
    "             'accum solid precip in last 10 days', 'total precip last 10 days', 'average temp last 6 days']\n",
    "MLP_train = train[variables]\n",
    "MLP_val = val[variables]\n",
    "# delete all rows with nan values \n",
    "MLP_train = MLP_train.dropna()\n",
    "MLP_val = MLP_val.dropna()\n",
    "# select explanatory and target variables \n",
    "y_train = MLP_train['SWE'].values.astype('float32')\n",
    "y_val = MLP_val['SWE'].values.astype('float32')    \n",
    "x_train = MLP_train.drop('SWE', axis=1).values.astype('float32')\n",
    "x_val = MLP_val.drop('SWE', axis=1).values.astype('float32')\n",
    "\n",
    "# generate folder for saved MLP networks and results \n",
    "save_path =  '01_savedMLP_ProjectKON/Canada'\n",
    "if os.path.exists(save_path):\n",
    "    shutil.rmtree(save_path)\n",
    "os.makedirs(save_path)\n",
    "save_path_results = '{0}/results'.format(save_path)\n",
    "os.makedirs(save_path_results)\n",
    "save_path_MLPs = '{0}/saved_MLPs'.format(save_path)\n",
    "os.makedirs(save_path_MLPs)\n",
    "\n",
    "\n",
    "# determination of MLP setup \n",
    "nb_members = 20 \n",
    "nb_hid = 120\n",
    "nb_epochs = 5\n",
    "activ_fc = 'tanh'\n",
    "init_w = 2\n",
    "init_b = 2\n",
    "optAlg = 'Adadelta'\n",
    "batch_size = 100\n",
    "shuf_data = 1\n",
    "\n",
    "# optimise MLP ensmeble by function; trained MLP networks and scaler for standardisation are saved to save_path\n",
    "MLP(x_train, x_val, y_train, y_val, nb_epochs, nb_members, nb_hid, save_path_MLPs,\n",
    "    activ_fc, init_w, init_b, optAlg, batch_size, shuf_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----------------------------------------------------------------------------------------------------------------------------------\n",
    "## Evaluation of training \n",
    "### 3a. Function for ensemble evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import os\n",
    "from PerformanceFunctions import CRPS, DecompCRPS, Reliability, ScoreLog, TalagrandRank\n",
    "\n",
    "def performance(ensemble, obs, filename_fig):\n",
    "\n",
    "    # calculate the MAE, RMSE and MBE of median \n",
    "    # calculate the median\n",
    "    median = np.nanmedian(ensemble, axis=1)\n",
    "    # calculate MAE, MBE and RSME between median and validation\n",
    "    MAE = np.nanmean(np.absolute(median - obs)) \n",
    "    RMSE = np.sqrt(np.nanmean((median - obs) ** 2))\n",
    "    MBE = np.nanmean(median - obs)\n",
    "    \n",
    "    # histogram of simulated medians and the validation dataset\n",
    "    max_median = median.max()\n",
    "    max_obs = obs.max()\n",
    "    max_all = np.array([max_median, max_obs]).max()\n",
    "    bin_nb = int(max_all/40)\n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    ax.hist([median, obs],\n",
    "             bins=bin_nb,\n",
    "             color=['red', 'blue'],\n",
    "             label=['median simulation', 'observation'])\n",
    "    ax.set_xlabel('value of SWE in $mm$')\n",
    "    ax.set_ylabel('count of sim/obs')\n",
    "    ax.legend(loc='upper right')\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # save figure\n",
    "    strFile = filename_fig + '/histogram.png'\n",
    "    if os.path.isfile(strFile):\n",
    "        os.remove(strFile)\n",
    "    fig.savefig(strFile)\n",
    "    plt.close()\n",
    "    \n",
    "    # calculate the CRPS by using the empirical case\n",
    "    CRPS_emp = CRPS.CRPS(ensemble, obs, 'emp')\n",
    "    \n",
    "    # decompose the CRPS \n",
    "    total, reliability, potential = DecompCRPS.decomp_CRPS(ensemble, obs)\n",
    "    \n",
    "    # get reliability diagram \n",
    "    nomi, Meff, Mlen = Reliability.Reliability(ensemble, obs)\n",
    "    # creat diagram and save it \n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    ax.plot(nomi, Meff, color='black', marker='o', linestyle='None')\n",
    "    ax.plot([0,1], [0,1], color='red',  linestyle='dashed')\n",
    "    ax.set_xlabel('forecast probabilities')\n",
    "    ax.set_ylabel('observed relat. freq.')\n",
    "    plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    # save figure\n",
    "    strFile = filename_fig + '/reliabilitiy_diagramm.png'\n",
    "    if os.path.isfile(strFile):\n",
    "        os.remove(strFile)\n",
    "    fig.savefig(strFile)\n",
    "    plt.close()\n",
    "    \n",
    "    # get the log/ignorance score\n",
    "    S_LOG_emp, ind_miss_emp = ScoreLog.score_log(ensemble, obs, 'Empirical', thres=0.001)\n",
    "    \n",
    "    # calculate the Talagrand Rank histogramm\n",
    "    hist, ranks = TalagrandRank.Talagrand_rank(ensemble, obs)\n",
    "    # creat diagram and save it \n",
    "    x = np.arange(len(hist))\n",
    "    fig, ax = plt.subplots(figsize=(10, 7))\n",
    "    plt.bar(x, hist)\n",
    "    ax.set_xticks([0, 4, 9, 14, 20])\n",
    "    ax.set_xticklabels([1, 5, 10, 15, 21])\n",
    "    ax.set_xlabel('rank of observation')\n",
    "    ax.set_ylabel('observed freq.')\n",
    "    plt.tight_layout()\n",
    "\n",
    "    # save figure\n",
    "    strFile = filename_fig + '/Talagrand_diagramm.png'\n",
    "    if os.path.isfile(strFile):\n",
    "        os.remove(strFile)\n",
    "    fig.savefig(strFile)\n",
    "    plt.close()\n",
    "\n",
    "    # save results to csv file\n",
    "    results = [MAE, RMSE, MBE, CRPS_emp, reliability, potential, S_LOG_emp]   \n",
    "    results_pd = pd.DataFrame(results, index=['MAE', 'RMSE', 'MBE', 'CRPS emp',\n",
    "                                              'reliability', 'potential', 'Ignorance score emp'])\n",
    "    results_pd.to_csv(filename_fig + '/results.csv', index=True)\n",
    "    \n",
    "    print('performance analysis finished')\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3b. Evaluate on vaildation data set\n",
    "\n",
    "#### Perturbe SD on validation data set\n",
    "Trained MLPs are loaded; Take care when loaded MLP is used, order of input varibales needs to be indentical to the order of the input varibles in training! 20 perturbed SD on the validation data set are simulated, resulting in 20 members for each MLP, entailing in total 400 memebers over the 20 MLPs; 20 equally distributed quantiles define the new 20 members;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\konst\\anaconda3\\envs\\test4\\lib\\site-packages\\tensorflow\\python\\training\\saver.py:1266: checkpoint_exists (from tensorflow.python.training.checkpoint_management) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use standard file APIs to check for files with this prefix.\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Canada/saved_MLPs/mb_0.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Canada/saved_MLPs/mb_1.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Canada/saved_MLPs/mb_2.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Canada/saved_MLPs/mb_3.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Canada/saved_MLPs/mb_4.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Canada/saved_MLPs/mb_5.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Canada/saved_MLPs/mb_6.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Canada/saved_MLPs/mb_7.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Canada/saved_MLPs/mb_8.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Canada/saved_MLPs/mb_9.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Canada/saved_MLPs/mb_10.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Canada/saved_MLPs/mb_11.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Canada/saved_MLPs/mb_12.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Canada/saved_MLPs/mb_13.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Canada/saved_MLPs/mb_14.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Canada/saved_MLPs/mb_15.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Canada/saved_MLPs/mb_16.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Canada/saved_MLPs/mb_17.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Canada/saved_MLPs/mb_18.ckpt\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Canada/saved_MLPs/mb_19.ckpt\n"
     ]
    }
   ],
   "source": [
    "# load scaler for standardisation         \n",
    "[scalerIn, scalerOut] = pickle.load(open(save_path_MLPs + '/scaler', \"rb\"))\n",
    "\n",
    "# load validation data set\n",
    "val = pickle.load(open(\"00_saved_variables/data_val\", \"rb\"))\n",
    "\n",
    "# determine variables used for MLP optimisation \n",
    "variables = ['SD', 'SWE', 'Day of year', 'days without snow', 'number frost-defrost',\n",
    "             'accum pos degrees', 'average age SC', 'number layer', 'accum solid precip',\n",
    "             'accum solid precip in last 10 days', 'total precip last 10 days', 'average temp last 6 days']\n",
    "MLP_val = val[variables]\n",
    "# delete all rows with nan values \n",
    "MLP_val = MLP_val.dropna()\n",
    "# select explanatory and target variables \n",
    "y_val = MLP_val['SWE'].values.astype('float32')    \n",
    "x_val = MLP_val.drop('SWE', axis=1).values.astype('float32')\n",
    "# standardise input \n",
    "x_val_std = scalerIn.transform(x_val)\n",
    "# find index for SD for perturbation \n",
    "idx_SD = MLP_val.drop('SWE', axis=1).columns.get_loc('SD')\n",
    "\n",
    "# initialise matrix for ensemble with 400 members\n",
    "ensemble400 = np.empty((len(y_val), 20*20))\n",
    "\n",
    "# assign model setup \n",
    "nb_members = 20 \n",
    "\n",
    "for mb in range(nb_members):\n",
    "    # create network graph\n",
    "    tf.reset_default_graph()\n",
    "    imported_graph = tf.train.import_meta_graph(save_path_MLPs +  \"/mb_{0}.ckpt.meta\".format(mb))\n",
    "    with tf.Session() as sess:\n",
    "        # restore parameter\n",
    "        imported_graph.restore(sess, save_path_MLPs +  \"/mb_{0}.ckpt\".format(mb))\n",
    "        \n",
    "        # get prediction with noisy inputs as an ensemble \n",
    "        for k in range(x_val.shape[0]):\n",
    "            line_input = x_val[k, :]\n",
    "            input_net = np.tile(line_input, (20, 1))\n",
    "            SD = line_input[idx_SD]\n",
    "            if SD < 20: \n",
    "                SD_low = SD - 1\n",
    "                SD_high = SD + 1\n",
    "            else: \n",
    "                SD_low = SD * 0.95\n",
    "                SD_high = SD * 1.05\n",
    "            SD_noise_1rec = np.random.uniform(low=SD_low, high=SD_high, size=20)\n",
    "            input_net[:, idx_SD] = SD_noise_1rec\n",
    "            input_net_std = scalerIn.transform(input_net)\n",
    "            predict_std = sess.run(\"op_to_restore:0\", feed_dict={\"input:0\": input_net_std}).flatten()\n",
    "            ensemble400[k, mb*20:(mb+1)*20] = scalerOut.inverse_transform(predict_std).flatten()   \n",
    "\n",
    "# determine 20 quantiles, to get 20 members \n",
    "ensemble20 = np.quantile(ensemble400, np.arange(0.025, 1, 1/20), axis=1).transpose()\n",
    "\n",
    "# save ensemble          \n",
    "pickle.dump(ensemble20, open(save_path_results + '/ensembleVal_SD_pt', \"wb\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Apply performance function \n",
    "The performance function calculates the MAE, RMSE, MBE, CRPS, its decomposition into reliability and potential CRPS and the ignorance score. Furthermore, a histogram which compares the median simulation of the ensemble with the observation, the reliability diagram and the Talagrand histogram are plotted and saved."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bins used for empirical method determined by ensemble members\n",
      "performance analysis finished\n"
     ]
    }
   ],
   "source": [
    "from performance import performance\n",
    "\n",
    "\n",
    "# load ensemble \n",
    "ensemble = pickle.load(open(save_path_results + '/ensembleVal_SD_pt', \"rb\"))\n",
    "\n",
    "# load observations \n",
    "val = pickle.load(open(\"00_saved_variables/data_val\", \"rb\"))\n",
    "# determine variables used for MLP optimisation \n",
    "variables = ['SD', 'SWE', 'Day of year', 'days without snow', 'number frost-defrost',\n",
    "             'accum pos degrees', 'average age SC', 'number layer', 'accum solid precip',\n",
    "             'accum solid precip in last 10 days', 'total precip last 10 days', 'average temp last 6 days']\n",
    "MLP_val = val[variables]\n",
    "# delete all rows with nan values \n",
    "MLP_val = MLP_val.dropna()\n",
    "# select target variable (observation)\n",
    "obs = MLP_val['SWE'].values.astype('float32') \n",
    "    \n",
    "# apply performance function, saves graphics and results (as csv) in folder assigned above\n",
    "performance(ensemble, obs, save_path_results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----------------------------------------------------------------------------------------------------------------------------------\n",
    "## 4. Multiple MLP model\n",
    "Same procedure was done for each snow class by cutting the training data set into the different snow class and training one ensemble of 20 MLPs for each snow class. In 2a. function for training of an ensemble of MLPs, a saver object is created by:  \n",
    "    - saver = tf.train.Saver(max_to_keep=nb_members)\n",
    "    \n",
    "This saver object is related to the network structure. The networks for different snow classes differ in the number of training epochs and number of neurons in the hidden layer. Constructing the saver object in a loop over the snow classes does not save the correct networks. The saver object is only allowed to be called once! Therefore, the code need to run for each snow class individually with closing the IPhython console between each snow class. (There must be a way to save different networks structure in a loop, but I could not find it out. I am sorry!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ----------------------------------------------------------------------------------------------------------------------------------\n",
    "## 5. Simulation and evaluation on unseen data set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model: \n",
    "The model uses an ensemble of 20 multi-layer perceptrons (MLP). Each MLP contains a single hidden layer. 11 inputs are taken. One is SD, the remaining varibles are calculated from total precipitation, maxiaml and minimal temperature. (The input are defined in the paper, except snow density from ERA5 is excluded.) SWE is the output varible of the MLP. With 20 MLPs, 20 SWE outputs are precessed, resulting in a probabilistic simulation of SWE. The two following models have been trained. \n",
    "    1. Single MLP model: The first model uses the training data set over entire Canada for training the MLPs. \n",
    "       (120 hidden neurons, 5 epochs of training)\n",
    "    2. Multiple MLP model: One ensemble of MLPs is trained for each snow class. Snow classes are defined by\n",
    "       Sturm et al. (2009) https://doi.org/10.5065/D69G5JX5\n",
    "       (different number of hidden neurons and number of epochs for different snow classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5a. Simulate SWE \n",
    "Simulation is done on unseen testing data set. It can be decided between the two models 'SingleMLP' and 'MultipleMLP'. Trained MLPs are loaded; Take care that order of input varibales needs to be indentical to the order of the input varibles in training! 20 perturbed SD on the validation data set are simulated, resulting in 20 members for each MLP, entailing in total 400 memebers over the 20 MLPs; 20 equally distributed quantiles define the new 20 members;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restoring member 0\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Tundra snow/saved_MLPs/mb_0.ckpt\n",
      "Restoring member 1\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Tundra snow/saved_MLPs/mb_1.ckpt\n",
      "Restoring member 2\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Tundra snow/saved_MLPs/mb_2.ckpt\n",
      "Restoring member 3\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Tundra snow/saved_MLPs/mb_3.ckpt\n",
      "Restoring member 4\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Tundra snow/saved_MLPs/mb_4.ckpt\n",
      "Restoring member 5\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Tundra snow/saved_MLPs/mb_5.ckpt\n",
      "Restoring member 6\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Tundra snow/saved_MLPs/mb_6.ckpt\n",
      "Restoring member 7\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Tundra snow/saved_MLPs/mb_7.ckpt\n",
      "Restoring member 8\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Tundra snow/saved_MLPs/mb_8.ckpt\n",
      "Restoring member 9\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Tundra snow/saved_MLPs/mb_9.ckpt\n",
      "Restoring member 10\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Tundra snow/saved_MLPs/mb_10.ckpt\n",
      "Restoring member 11\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Tundra snow/saved_MLPs/mb_11.ckpt\n",
      "Restoring member 12\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Tundra snow/saved_MLPs/mb_12.ckpt\n",
      "Restoring member 13\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Tundra snow/saved_MLPs/mb_13.ckpt\n",
      "Restoring member 14\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Tundra snow/saved_MLPs/mb_14.ckpt\n",
      "Restoring member 15\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Tundra snow/saved_MLPs/mb_15.ckpt\n",
      "Restoring member 16\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Tundra snow/saved_MLPs/mb_16.ckpt\n",
      "Restoring member 17\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Tundra snow/saved_MLPs/mb_17.ckpt\n",
      "Restoring member 18\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Tundra snow/saved_MLPs/mb_18.ckpt\n",
      "Restoring member 19\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Tundra snow/saved_MLPs/mb_19.ckpt\n",
      "Restoring member 0\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Taiga snow/saved_MLPs/mb_0.ckpt\n",
      "Restoring member 1\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Taiga snow/saved_MLPs/mb_1.ckpt\n",
      "Restoring member 2\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Taiga snow/saved_MLPs/mb_2.ckpt\n",
      "Restoring member 3\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Taiga snow/saved_MLPs/mb_3.ckpt\n",
      "Restoring member 4\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Taiga snow/saved_MLPs/mb_4.ckpt\n",
      "Restoring member 5\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Taiga snow/saved_MLPs/mb_5.ckpt\n",
      "Restoring member 6\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Taiga snow/saved_MLPs/mb_6.ckpt\n",
      "Restoring member 7\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Taiga snow/saved_MLPs/mb_7.ckpt\n",
      "Restoring member 8\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Taiga snow/saved_MLPs/mb_8.ckpt\n",
      "Restoring member 9\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Taiga snow/saved_MLPs/mb_9.ckpt\n",
      "Restoring member 10\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Taiga snow/saved_MLPs/mb_10.ckpt\n",
      "Restoring member 11\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Taiga snow/saved_MLPs/mb_11.ckpt\n",
      "Restoring member 12\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Taiga snow/saved_MLPs/mb_12.ckpt\n",
      "Restoring member 13\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Taiga snow/saved_MLPs/mb_13.ckpt\n",
      "Restoring member 14\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Taiga snow/saved_MLPs/mb_14.ckpt\n",
      "Restoring member 15\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Taiga snow/saved_MLPs/mb_15.ckpt\n",
      "Restoring member 16\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Taiga snow/saved_MLPs/mb_16.ckpt\n",
      "Restoring member 17\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Taiga snow/saved_MLPs/mb_17.ckpt\n",
      "Restoring member 18\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Taiga snow/saved_MLPs/mb_18.ckpt\n",
      "Restoring member 19\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Taiga snow/saved_MLPs/mb_19.ckpt\n",
      "Restoring member 0\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Maritime snow/saved_MLPs/mb_0.ckpt\n",
      "Restoring member 1\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Maritime snow/saved_MLPs/mb_1.ckpt\n",
      "Restoring member 2\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Maritime snow/saved_MLPs/mb_2.ckpt\n",
      "Restoring member 3\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Maritime snow/saved_MLPs/mb_3.ckpt\n",
      "Restoring member 4\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Maritime snow/saved_MLPs/mb_4.ckpt\n",
      "Restoring member 5\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Maritime snow/saved_MLPs/mb_5.ckpt\n",
      "Restoring member 6\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Maritime snow/saved_MLPs/mb_6.ckpt\n",
      "Restoring member 7\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Maritime snow/saved_MLPs/mb_7.ckpt\n",
      "Restoring member 8\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Maritime snow/saved_MLPs/mb_8.ckpt\n",
      "Restoring member 9\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Maritime snow/saved_MLPs/mb_9.ckpt\n",
      "Restoring member 10\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Maritime snow/saved_MLPs/mb_10.ckpt\n",
      "Restoring member 11\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Maritime snow/saved_MLPs/mb_11.ckpt\n",
      "Restoring member 12\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Maritime snow/saved_MLPs/mb_12.ckpt\n",
      "Restoring member 13\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Maritime snow/saved_MLPs/mb_13.ckpt\n",
      "Restoring member 14\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Maritime snow/saved_MLPs/mb_14.ckpt\n",
      "Restoring member 15\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Maritime snow/saved_MLPs/mb_15.ckpt\n",
      "Restoring member 16\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Maritime snow/saved_MLPs/mb_16.ckpt\n",
      "Restoring member 17\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Maritime snow/saved_MLPs/mb_17.ckpt\n",
      "Restoring member 18\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Maritime snow/saved_MLPs/mb_18.ckpt\n",
      "Restoring member 19\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Maritime snow/saved_MLPs/mb_19.ckpt\n",
      "Restoring member 0\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Ephemeral snow/saved_MLPs/mb_0.ckpt\n",
      "Restoring member 1\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Ephemeral snow/saved_MLPs/mb_1.ckpt\n",
      "Restoring member 2\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Ephemeral snow/saved_MLPs/mb_2.ckpt\n",
      "Restoring member 3\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Ephemeral snow/saved_MLPs/mb_3.ckpt\n",
      "Restoring member 4\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Ephemeral snow/saved_MLPs/mb_4.ckpt\n",
      "Restoring member 5\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Ephemeral snow/saved_MLPs/mb_5.ckpt\n",
      "Restoring member 6\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Ephemeral snow/saved_MLPs/mb_6.ckpt\n",
      "Restoring member 7\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Ephemeral snow/saved_MLPs/mb_7.ckpt\n",
      "Restoring member 8\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Ephemeral snow/saved_MLPs/mb_8.ckpt\n",
      "Restoring member 9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Ephemeral snow/saved_MLPs/mb_9.ckpt\n",
      "Restoring member 10\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Ephemeral snow/saved_MLPs/mb_10.ckpt\n",
      "Restoring member 11\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Ephemeral snow/saved_MLPs/mb_11.ckpt\n",
      "Restoring member 12\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Ephemeral snow/saved_MLPs/mb_12.ckpt\n",
      "Restoring member 13\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Ephemeral snow/saved_MLPs/mb_13.ckpt\n",
      "Restoring member 14\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Ephemeral snow/saved_MLPs/mb_14.ckpt\n",
      "Restoring member 15\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Ephemeral snow/saved_MLPs/mb_15.ckpt\n",
      "Restoring member 16\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Ephemeral snow/saved_MLPs/mb_16.ckpt\n",
      "Restoring member 17\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Ephemeral snow/saved_MLPs/mb_17.ckpt\n",
      "Restoring member 18\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Ephemeral snow/saved_MLPs/mb_18.ckpt\n",
      "Restoring member 19\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Ephemeral snow/saved_MLPs/mb_19.ckpt\n",
      "Restoring member 0\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Prairie snow/saved_MLPs/mb_0.ckpt\n",
      "Restoring member 1\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Prairie snow/saved_MLPs/mb_1.ckpt\n",
      "Restoring member 2\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Prairie snow/saved_MLPs/mb_2.ckpt\n",
      "Restoring member 3\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Prairie snow/saved_MLPs/mb_3.ckpt\n",
      "Restoring member 4\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Prairie snow/saved_MLPs/mb_4.ckpt\n",
      "Restoring member 5\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Prairie snow/saved_MLPs/mb_5.ckpt\n",
      "Restoring member 6\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Prairie snow/saved_MLPs/mb_6.ckpt\n",
      "Restoring member 7\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Prairie snow/saved_MLPs/mb_7.ckpt\n",
      "Restoring member 8\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Prairie snow/saved_MLPs/mb_8.ckpt\n",
      "Restoring member 9\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Prairie snow/saved_MLPs/mb_9.ckpt\n",
      "Restoring member 10\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Prairie snow/saved_MLPs/mb_10.ckpt\n",
      "Restoring member 11\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Prairie snow/saved_MLPs/mb_11.ckpt\n",
      "Restoring member 12\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Prairie snow/saved_MLPs/mb_12.ckpt\n",
      "Restoring member 13\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Prairie snow/saved_MLPs/mb_13.ckpt\n",
      "Restoring member 14\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Prairie snow/saved_MLPs/mb_14.ckpt\n",
      "Restoring member 15\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Prairie snow/saved_MLPs/mb_15.ckpt\n",
      "Restoring member 16\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Prairie snow/saved_MLPs/mb_16.ckpt\n",
      "Restoring member 17\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Prairie snow/saved_MLPs/mb_17.ckpt\n",
      "Restoring member 18\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Prairie snow/saved_MLPs/mb_18.ckpt\n",
      "Restoring member 19\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Prairie snow/saved_MLPs/mb_19.ckpt\n",
      "Restoring member 0\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Mountain snow/saved_MLPs/mb_0.ckpt\n",
      "Restoring member 1\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Mountain snow/saved_MLPs/mb_1.ckpt\n",
      "Restoring member 2\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Mountain snow/saved_MLPs/mb_2.ckpt\n",
      "Restoring member 3\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Mountain snow/saved_MLPs/mb_3.ckpt\n",
      "Restoring member 4\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Mountain snow/saved_MLPs/mb_4.ckpt\n",
      "Restoring member 5\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Mountain snow/saved_MLPs/mb_5.ckpt\n",
      "Restoring member 6\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Mountain snow/saved_MLPs/mb_6.ckpt\n",
      "Restoring member 7\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Mountain snow/saved_MLPs/mb_7.ckpt\n",
      "Restoring member 8\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Mountain snow/saved_MLPs/mb_8.ckpt\n",
      "Restoring member 9\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Mountain snow/saved_MLPs/mb_9.ckpt\n",
      "Restoring member 10\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Mountain snow/saved_MLPs/mb_10.ckpt\n",
      "Restoring member 11\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Mountain snow/saved_MLPs/mb_11.ckpt\n",
      "Restoring member 12\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Mountain snow/saved_MLPs/mb_12.ckpt\n",
      "Restoring member 13\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Mountain snow/saved_MLPs/mb_13.ckpt\n",
      "Restoring member 14\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Mountain snow/saved_MLPs/mb_14.ckpt\n",
      "Restoring member 15\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Mountain snow/saved_MLPs/mb_15.ckpt\n",
      "Restoring member 16\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Mountain snow/saved_MLPs/mb_16.ckpt\n",
      "Restoring member 17\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Mountain snow/saved_MLPs/mb_17.ckpt\n",
      "Restoring member 18\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Mountain snow/saved_MLPs/mb_18.ckpt\n",
      "Restoring member 19\n",
      "INFO:tensorflow:Restoring parameters from 01_savedMLP_ProjectKON/Mountain snow/saved_MLPs/mb_19.ckpt\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "import pickle \n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "# load snowclasses and change projection to WGS84 (unit in lat/long)\n",
    "SC_Canada = pickle.load(open(\"00_saved_variables/SC_Canada\", \"rb\"))\n",
    "\n",
    "# switch between the two models\n",
    "model = 'MultipleMLP'\n",
    "\n",
    "# decision between the two models 'SingleMLP' and 'MultipleMLP'\n",
    "if model == 'SingleMLP': \n",
    "    snowclasses = ['Canada']\n",
    "if model == 'MultipleMLP':   \n",
    "    snowclasses = SC_Canada['snow type']\n",
    "\n",
    "# load testing data set \n",
    "data_test = pickle.load(open('00_saved_variables/data_test', \"rb\"))\n",
    "# determine variables used for MLP optimisation \n",
    "variables = ['SD', 'SWE', 'Day of year', 'days without snow', 'number frost-defrost',\n",
    "             'accum pos degrees', 'average age SC', 'number layer', 'accum solid precip',\n",
    "             'accum solid precip in last 10 days', 'total precip last 10 days', 'average temp last 6 days']\n",
    "# delete all rows with nan values \n",
    "MLP_test = data_test[variables]\n",
    "MLP_test = MLP_test.dropna()\n",
    "MLP_test_input = MLP_test.drop('SWE', axis=1)\n",
    "\n",
    "# preparation DataFrame for export \n",
    "sim = pd.DataFrame(np.empty((len(MLP_test), 20)), index=MLP_test.index)\n",
    "\n",
    "\n",
    "for sn_class in snowclasses:    \n",
    "    if sn_class == 'Canada': \n",
    "        input_SC = MLP_test_input\n",
    "    else:\n",
    "        # get data for certain snow class\n",
    "        input_SC = MLP_test_input.loc[MLP_data['snowclass'] == sn_class]\n",
    "\n",
    "    # condiiton if snow class equal 'Water' or 'Ice', no data points in the data set (further explanation in data preparation\n",
    "    # in SingleMLP_train)\n",
    "    if input_SC.shape[0] == 0: \n",
    "        continue\n",
    "\n",
    "    # find index for SD for perturbation later\n",
    "    idx_SD = input_SC.columns.get_loc('SD')\n",
    "\n",
    "    # input needs to be in nd-array\n",
    "    x_test = input_SC.values.astype('float32')\n",
    "\n",
    "    # path to saved MLPs and scaler\n",
    "    save_path = '01_savedMLP_ProjectKON/{0}/saved_MLPs'.format(sn_class)\n",
    "\n",
    "    # standardize data \n",
    "    # sclaer In and Out and stadardise\n",
    "    scalerIn, scalerOut = pickle.load(open(save_path + \"/scaler\", \"rb\"))\n",
    "    x_test_std = scalerIn.transform(x_test)\n",
    "\n",
    "    # initialise matrix for simulation\n",
    "    ensemble400 = np.empty((x_test_std.shape[0], 20*20))\n",
    "\n",
    "    for mb in range(20):\n",
    "        print('Restoring member {0}'.format(mb))\n",
    "        tf.reset_default_graph()\n",
    "        imported_graph = tf.train.import_meta_graph(save_path + \"/mb_{0}.ckpt.meta\".format(mb))\n",
    "        with tf.Session() as sess:\n",
    "            # restore parameter\n",
    "            imported_graph.restore(sess, save_path +  \"/mb_{0}.ckpt\".format(mb))\n",
    "\n",
    "            # get prediction with noisy inputs as an ensemble \n",
    "            for k in range(x_test.shape[0]):\n",
    "                line_input = x_test[k, :]\n",
    "                input_net = np.tile(line_input, (20, 1))\n",
    "                SD = line_input[idx_SD]\n",
    "                if SD < 20: \n",
    "                    SD_low = SD - 1\n",
    "                    SD_high = SD + 1\n",
    "                else: \n",
    "                    SD_low = SD * 0.95\n",
    "                    SD_high = SD * 1.05\n",
    "                SD_noise_1rec = np.random.uniform(low=SD_low, high=SD_high, size=20)\n",
    "                input_net[:, idx_SD] = SD_noise_1rec\n",
    "                input_net_std = scalerIn.transform(input_net)\n",
    "                predict_std = sess.run(\"op_to_restore:0\", feed_dict={\"input:0\": input_net_std}).flatten()\n",
    "                ensemble400[k, mb*20:(mb+1)*20] = scalerOut.inverse_transform(predict_std).flatten()   \n",
    "\n",
    "    # determine 20 quantiles, to get 20 members \n",
    "    ensemble20 = np.quantile(ensemble400, np.arange(0.025, 1, 1/20), axis=1).transpose()\n",
    "\n",
    "    # save simulation in final DataFrame, which incluses all snow classes\n",
    "    sim.loc[input_SC.index, :] = ensemble20\n",
    "\n",
    "# folder for results on testing data set\n",
    "save_test = '02_results_testing/{0}'.format(model)\n",
    "if os.path.exists(save_test):\n",
    "    shutil.rmtree(save_test)\n",
    "os.makedirs(save_test)\n",
    "# save final simulation           \n",
    "pickle.dump(sim, open(save_test + '/ensembleTest_SD_pt', \"wb\"))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b. Evaluate simulation\n",
    "#### Apply performance function \n",
    "The performance function calculates the MAE, RMSE, MBE, CRPS, its decomposition into reliability and potential CRPS and the ignorance score. Furthermore, a histogram which compares the median simulation of the ensemble with the observation, the reliability diagram and the Talagrand histogram are plotted and saved. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bins used for empirical method determined by ensemble members\n",
      "performance analysis finished\n"
     ]
    }
   ],
   "source": [
    "# assign observations\n",
    "obs = MLP_test['SWE'].values.astype('float32') \n",
    "    \n",
    "# apply performance function, saves graphics and results (as csv) in folder assigned above\n",
    "performance(sim, obs, save_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SD2SWE",
   "language": "python",
   "name": "sd2swe"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
